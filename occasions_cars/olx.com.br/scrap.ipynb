{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de détails récupérés : 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import csv\n",
    "\n",
    "def get_car_links(main_url, page_number):\n",
    "    try:\n",
    "        url = f\"{main_url}?o={page_number}\"\n",
    "        response = requests.get(url, headers={'User-agent': 'Mozilla/5.0'})\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = bs(response.text, 'html.parser')\n",
    "        car_links = [a['href'] for a in soup.select('.DS-NewAdCard-Link')]\n",
    "        return car_links\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors de la récupération des liens de la page {page_number} : {e}\")\n",
    "        return []\n",
    "\n",
    "def scrape_car_details(car_url):\n",
    "    try:\n",
    "        response = requests.get(car_url, headers={'User-agent': 'Mozilla/5.0'})\n",
    "        response.raise_for_status()\n",
    "\n",
    "        car_soup = bs(response.text, 'html.parser')\n",
    "        car_details = car_soup.find_all('.ad__sc-h3us20-6.ikRegS')\n",
    "\n",
    "        # Processus des données récupérées\n",
    "        details_text = '\\n'.join(detail.text for detail in car_details)\n",
    "        print(details_text)\n",
    "\n",
    "        return details_text\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors de la récupération des détails de la voiture : {e}\")\n",
    "        return None\n",
    "\n",
    "main_page_url = 'https://www.olx.com.br/autos-e-pecas/carros-vans-e-utilitarios'\n",
    "total_pages = 10  # Modifier le nombre total de pages selon vos besoins\n",
    "car_details_list = []\n",
    "\n",
    "for page_number in range(1, total_pages + 1):\n",
    "    car_links = get_car_links(main_page_url, page_number)\n",
    "\n",
    "    for car_link in car_links:\n",
    "        print(f\"Scrapping details for car: {car_link}\")\n",
    "        car_detail = scrape_car_details(car_link)\n",
    "        \n",
    "        if car_detail:\n",
    "            car_details_list.append(car_detail)\n",
    "        \n",
    "        time.sleep(2)  # Respectez les politiques du site pour éviter d'être bloqué\n",
    "\n",
    "# Ajout de débogage\n",
    "print(\"Nombre total de détails récupérés :\", len(car_details_list))\n",
    "\n",
    "# Écrire les détails des voitures dans un fichier CSV\n",
    "with open('car_details.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Car Details'])\n",
    "    \n",
    "    for detail in car_details_list:\n",
    "        csv_writer.writerow([detail])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
